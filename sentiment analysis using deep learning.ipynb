{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e4714b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07ff12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Kaggle dataset and place it in the current directory\n",
    "# the file is named 'movie_reviews.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691dbad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data.info()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6369f5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    cleaned_text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Remove punctuation and special characters\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
    "    # Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "data['cleaned_text'] = data['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e20e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        one of the other reviewers has mentioned that ...\n",
      "1        a wonderful little production the filming tech...\n",
      "2        i thought this was a wonderful way to spend ti...\n",
      "3        basically theres a family where a little boy j...\n",
      "4        petter matteis love in the time of money is a ...\n",
      "                               ...                        \n",
      "49995    i thought this movie did a down right good job...\n",
      "49996    bad plot bad dialogue bad acting idiotic direc...\n",
      "49997    i am a catholic taught in parochial elementary...\n",
      "49998    im going to have to disagree with the previous...\n",
      "49999    no one expects the star trek movies to be high...\n",
      "Name: cleaned_text, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4936245f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\deept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\deept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization and stopword removal\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopword_set = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14ceadf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stopword_set]\n",
    "    return tokens\n",
    "\n",
    "data['tokenized_text'] = data['cleaned_text'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd3a05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [one, reviewers, mentioned, watching, 1, oz, e...\n",
      "1        [wonderful, little, production, filming, techn...\n",
      "2        [thought, wonderful, way, spend, time, hot, su...\n",
      "3        [basically, theres, family, little, boy, jake,...\n",
      "4        [petter, matteis, love, time, money, visually,...\n",
      "                               ...                        \n",
      "49995    [thought, movie, right, good, job, wasnt, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [catholic, taught, parochial, elementary, scho...\n",
      "49998    [im, going, disagree, previous, comment, side,...\n",
      "49999    [one, expects, star, trek, movies, high, art, ...\n",
      "Name: tokenized_text, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c4b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\deept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\deept\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6191e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "data['lemmatized_text'] = data['tokenized_text'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c702c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [one, reviewer, mentioned, watching, 1, oz, ep...\n",
      "1        [wonderful, little, production, filming, techn...\n",
      "2        [thought, wonderful, way, spend, time, hot, su...\n",
      "3        [basically, there, family, little, boy, jake, ...\n",
      "4        [petter, matteis, love, time, money, visually,...\n",
      "                               ...                        \n",
      "49995    [thought, movie, right, good, job, wasnt, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [catholic, taught, parochial, elementary, scho...\n",
      "49998    [im, going, disagree, previous, comment, side,...\n",
      "49999    [one, expects, star, trek, movie, high, art, f...\n",
      "Name: lemmatized_text, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['lemmatized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adcacf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary creation\n",
    "vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['lemmatized_text'])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da8f3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization\n",
    "sequences = tokenizer.texts_to_sequences(data['lemmatized_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "642a84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence padding\n",
    "max_length = 100\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8d64c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Review:\n",
      " One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
      "\n",
      "Preprocessed Review:\n",
      " ['one', 'reviewer', 'mentioned', 'watching', '1', 'oz', 'episode', 'youll', 'hooked', 'right', 'exactly', 'happened', 'methe', 'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scene', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pull', 'punch', 'regard', 'drug', 'sex', 'violence', 'hardcore', 'classic', 'use', 'wordit', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focus', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'manyaryans', 'muslim', 'gangsta', 'latino', 'christian', 'italian', 'irish', 'moreso', 'scuffle', 'death', 'stare', 'dodgy', 'dealing', 'shady', 'agreement', 'never', 'far', 'awayi', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'go', 'show', 'wouldnt', 'dare', 'forget', 'pretty', 'picture', 'painted', 'mainstream', 'audience', 'forget', 'charm', 'forget', 'romanceoz', 'doesnt', 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'couldnt', 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'level', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guard', 'wholl', 'sold', 'nickel', 'inmate', 'wholl', 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmate', 'turned', 'prison', 'bitch', 'due', 'lack', 'street', 'skill', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewingthats', 'get', 'touch', 'darker', 'side']\n",
      "\n",
      "Padded Sequence:\n",
      " [   4 1020  944   67  409 3098  175  368 2917  106  499  484 6869   24\n",
      "   27 2984 3098 4984    1   16  486  129  106  252   32 1573   23 6291\n",
      " 5231    1   23  905 2034 2051  638  282  486 3149  228  253    1  372\n",
      " 3098 9028  256    1 6206 2356  589    1  754 1247    1  399 4315 1961\n",
      " 1041 1962 1823  817  243    1    1  231 4125 3403  399  245    1 3807\n",
      "    1 6687 1186  866 2273    1    1  216 3828 6688 1653 7741 7418   42\n",
      "  137    1   12   41  186 1061   23  550   89   32   23  453 2422  701\n",
      "   95  250]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "#train_ratio = 0.8\n",
    "#train_size = int(train_ratio * len(data))\n",
    "\n",
    "#x_train = padded_sequences[:train_size]\n",
    "#y_train = data['sentiment'][:train_size]\n",
    "\n",
    "#x_test = padded_sequences[train_size:]\n",
    "#y_test = data['sentiment'][train_size:]\n",
    "\n",
    "# Print a sample preprocessed review\n",
    "#print('Original Review:\\n', data['review'][0])\n",
    "\n",
    "#print('\\nPreprocessed Review:\\n', data['lemmatized_text'][0])\n",
    "\n",
    "#print('\\nPadded Sequence:\\n', x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14eacb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 100, 100)          1000000   \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,125,569\n",
      "Trainable params: 1,125,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define the word embedding and sentiment analysis model\n",
    "embedding_dim = 100\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f03f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 274s 430ms/step - loss: 0.4572 - accuracy: 0.8030 - val_loss: 0.3501 - val_accuracy: 0.8622\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 0.3501 - accuracy: 0.8622\n",
      "Test Loss: 0.35014477372169495\n",
      "Test Accuracy: 0.8622000217437744\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "# Split the dataset into training and testing sets\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(data))\n",
    "\n",
    "x_train = padded_sequences[:train_size]\n",
    "y_train = data['sentiment'][:train_size]\n",
    "\n",
    "x_test = padded_sequences[train_size:]\n",
    "y_test = data['sentiment'][train_size:]\n",
    "\n",
    "# Convert labels to numerical format\n",
    "label_mapping = {'positive': 1, 'negative': 0}\n",
    "y_train = np.array([label_mapping.get(label, None) for label in y_train])\n",
    "y_test = np.array([label_mapping.get(label, None) for label in y_test])\n",
    "\n",
    "# Remove any None values from the lists\n",
    "x_train = x_train[np.array(y_train) != None]\n",
    "y_train = y_train[np.array(y_train) != None]\n",
    "x_test = x_test[np.array(y_test) != None]\n",
    "y_test = y_test[np.array(y_test) != None]\n",
    "\n",
    "# Convert the training data to NumPy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Define the model architecture\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length, trainable=True))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=1, batch_size=64, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "model.save('path/to/model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d31b6fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('path/to/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "054a7ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n",
      "Sentiment: positive\n",
      "Sentiment Scores: 0.77999425\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Preprocess the new text data\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    processed_text = \" \".join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Example new text\n",
    "new_text = \"This is an amazing show .\"\n",
    "\n",
    "# Preprocess the new text data\n",
    "new_text = preprocess_text(new_text)\n",
    "\n",
    "# Tokenize and pad the new text sequence\n",
    "new_sequence = tokenizer.texts_to_sequences([new_text])\n",
    "new_padded_sequence = pad_sequences(new_sequence, maxlen=max_length)\n",
    "\n",
    "\n",
    "# Predict the sentiment\n",
    "predictions = model.predict(new_padded_sequence)\n",
    "\n",
    "# Interpret the predictions\n",
    "sentiment_scores = predictions.squeeze()  # Remove any unnecessary dimensions\n",
    "sentiment = 'positive' if sentiment_scores > 0.5 else 'negative'\n",
    "\n",
    "# Print the result\n",
    "print('Sentiment:', sentiment)\n",
    "print('Sentiment Scores:', sentiment_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796c8cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
